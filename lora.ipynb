{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%autoreload` not found.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from functools import partial\n",
    "from model import Transformer, ModelArgs\n",
    "from lora import add_lora, remove_lora\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (tok_embeddings): Embedding(32000, 512)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (layers): ModuleList(\n",
      "    (0-7): 8 x TransformerBlock(\n",
      "      (attention): Attention(\n",
      "        (wq): Linear(in_features=512, out_features=512, bias=False)\n",
      "        (wk): Linear(in_features=512, out_features=512, bias=False)\n",
      "        (wv): Linear(in_features=512, out_features=512, bias=False)\n",
      "        (wo): Linear(in_features=512, out_features=512, bias=False)\n",
      "        (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (feed_forward): FeedForward(\n",
      "        (w1): Linear(in_features=512, out_features=1376, bias=False)\n",
      "        (w2): Linear(in_features=1376, out_features=512, bias=False)\n",
      "        (w3): Linear(in_features=512, out_features=1376, bias=False)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (attention_norm): RMSNorm()\n",
      "      (ffn_norm): RMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): RMSNorm()\n",
      "  (output): Linear(in_features=512, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# stories 42M\n",
    "dim = 512\n",
    "n_layers = 8\n",
    "n_heads = 8\n",
    "n_kv_heads = 8\n",
    "multiple_of = 32\n",
    "dropout = 0.0\n",
    "vocab_size = 32000\n",
    "max_seq_len = 1024\n",
    "\n",
    "model_args = dict(\n",
    "    dim=dim,\n",
    "    n_layers=n_layers,\n",
    "    n_heads=n_heads,\n",
    "    n_kv_heads=n_kv_heads,\n",
    "    vocab_size=vocab_size,\n",
    "    multiple_of=multiple_of,\n",
    "    max_seq_len=max_seq_len,\n",
    "    dropout=dropout,\n",
    ")\n",
    "\n",
    "gptconf = ModelArgs(**model_args)\n",
    "model = Transformer(gptconf)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add lora to layers.0.attention.wq\n",
      "add lora to layers.0.attention.wk\n",
      "add lora to layers.1.attention.wq\n",
      "add lora to layers.1.attention.wk\n",
      "add lora to layers.2.attention.wq\n",
      "add lora to layers.2.attention.wk\n",
      "add lora to layers.3.attention.wq\n",
      "add lora to layers.3.attention.wk\n",
      "add lora to layers.4.attention.wq\n",
      "add lora to layers.4.attention.wk\n",
      "add lora to layers.5.attention.wq\n",
      "add lora to layers.5.attention.wk\n",
      "add lora to layers.6.attention.wq\n",
      "add lora to layers.6.attention.wk\n",
      "add lora to layers.7.attention.wq\n",
      "add lora to layers.7.attention.wk\n",
      "Transformer(\n",
      "  (tok_embeddings): Embedding(32000, 512)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (layers): ModuleList(\n",
      "    (0-7): 8 x TransformerBlock(\n",
      "      (attention): Attention(\n",
      "        (wq): ParametrizedLinear(\n",
      "          in_features=512, out_features=512, bias=False\n",
      "          (parametrizations): ModuleDict(\n",
      "            (weight): ParametrizationList(\n",
      "              (0): LoRA(in_features=512, out_features=512, weight_type=linear, lora_rank=8, lora_alpha=16, lora_dropout_p=0.05\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (wk): ParametrizedLinear(\n",
      "          in_features=512, out_features=512, bias=False\n",
      "          (parametrizations): ModuleDict(\n",
      "            (weight): ParametrizationList(\n",
      "              (0): LoRA(in_features=512, out_features=512, weight_type=linear, lora_rank=8, lora_alpha=16, lora_dropout_p=0.05\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (wv): Linear(in_features=512, out_features=512, bias=False)\n",
      "        (wo): Linear(in_features=512, out_features=512, bias=False)\n",
      "        (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (feed_forward): FeedForward(\n",
      "        (w1): Linear(in_features=512, out_features=1376, bias=False)\n",
      "        (w2): Linear(in_features=1376, out_features=512, bias=False)\n",
      "        (w3): Linear(in_features=512, out_features=1376, bias=False)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (attention_norm): RMSNorm()\n",
      "      (ffn_norm): RMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): RMSNorm()\n",
      "  (output): Linear(in_features=512, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "lora_rank = 8\n",
    "lora_alpha = 16\n",
    "lora_dropout_p = 0.05\n",
    "target_modules = ['wq', 'wk']\n",
    "\n",
    "remove_lora(model)\n",
    "add_lora(model, rank=lora_rank, alpha=lora_alpha, dropout_p=lora_dropout_p, target_modules=target_modules)\n",
    "print(model)\n",
    "# register_lora_layer(model, lora_rank, lora_alpha, lora_dropout_p, target_modules)\n",
    "# model.apply(partial(register_lora_layer, lora_rank=lora_rank, lora_alpha=lora_alpha, lora_dropout_p=lora_dropout_p, target_modules=target_modules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lora_params(model, print_shapes=True):\n",
    "    def name_is_lora(name):\n",
    "        return (\n",
    "            len(name.split(\".\")) >= 4\n",
    "            and (name.split(\".\")[-4]) == \"parametrizations\"\n",
    "            and name.split(\".\")[-1] in [\"lora_A\", \"lora_B\"]\n",
    "        )\n",
    "    for n, p in model.named_parameters():\n",
    "        if name_is_lora(n):\n",
    "            if print_shapes:\n",
    "                print(n, p.shape)\n",
    "            yield p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.attention.wq.parametrizations.weight.0.lora_A torch.Size([512, 8])\n",
      "layers.0.attention.wq.parametrizations.weight.0.lora_B torch.Size([8, 512])\n",
      "layers.0.attention.wk.parametrizations.weight.0.lora_A torch.Size([512, 8])\n",
      "layers.0.attention.wk.parametrizations.weight.0.lora_B torch.Size([8, 512])\n",
      "layers.1.attention.wq.parametrizations.weight.0.lora_A torch.Size([512, 8])\n",
      "layers.1.attention.wq.parametrizations.weight.0.lora_B torch.Size([8, 512])\n",
      "layers.1.attention.wk.parametrizations.weight.0.lora_A torch.Size([512, 8])\n",
      "layers.1.attention.wk.parametrizations.weight.0.lora_B torch.Size([8, 512])\n",
      "layers.2.attention.wq.parametrizations.weight.0.lora_A torch.Size([512, 8])\n",
      "layers.2.attention.wq.parametrizations.weight.0.lora_B torch.Size([8, 512])\n",
      "layers.2.attention.wk.parametrizations.weight.0.lora_A torch.Size([512, 8])\n",
      "layers.2.attention.wk.parametrizations.weight.0.lora_B torch.Size([8, 512])\n",
      "layers.3.attention.wq.parametrizations.weight.0.lora_A torch.Size([512, 8])\n",
      "layers.3.attention.wq.parametrizations.weight.0.lora_B torch.Size([8, 512])\n",
      "layers.3.attention.wk.parametrizations.weight.0.lora_A torch.Size([512, 8])\n",
      "layers.3.attention.wk.parametrizations.weight.0.lora_B torch.Size([8, 512])\n",
      "layers.4.attention.wq.parametrizations.weight.0.lora_A torch.Size([512, 8])\n",
      "layers.4.attention.wq.parametrizations.weight.0.lora_B torch.Size([8, 512])\n",
      "layers.4.attention.wk.parametrizations.weight.0.lora_A torch.Size([512, 8])\n",
      "layers.4.attention.wk.parametrizations.weight.0.lora_B torch.Size([8, 512])\n",
      "layers.5.attention.wq.parametrizations.weight.0.lora_A torch.Size([512, 8])\n",
      "layers.5.attention.wq.parametrizations.weight.0.lora_B torch.Size([8, 512])\n",
      "layers.5.attention.wk.parametrizations.weight.0.lora_A torch.Size([512, 8])\n",
      "layers.5.attention.wk.parametrizations.weight.0.lora_B torch.Size([8, 512])\n",
      "layers.6.attention.wq.parametrizations.weight.0.lora_A torch.Size([512, 8])\n",
      "layers.6.attention.wq.parametrizations.weight.0.lora_B torch.Size([8, 512])\n",
      "layers.6.attention.wk.parametrizations.weight.0.lora_A torch.Size([512, 8])\n",
      "layers.6.attention.wk.parametrizations.weight.0.lora_B torch.Size([8, 512])\n",
      "layers.7.attention.wq.parametrizations.weight.0.lora_A torch.Size([512, 8])\n",
      "layers.7.attention.wq.parametrizations.weight.0.lora_B torch.Size([8, 512])\n",
      "layers.7.attention.wk.parametrizations.weight.0.lora_A torch.Size([512, 8])\n",
      "layers.7.attention.wk.parametrizations.weight.0.lora_B torch.Size([8, 512])\n"
     ]
    }
   ],
   "source": [
    "for n in get_lora_params(model):\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ckpt = torch.load(\"./out/tinyshakespeare_lora_stories260k_default_nodropout_0822_1536/ckpt.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'out_dir': 'out/tinyshakespeare_lora_stories260k_default_nodropout_0822_1536',\n",
       " 'eval_interval': 2000,\n",
       " 'log_interval': 1,\n",
       " 'eval_iters': 100,\n",
       " 'eval_only': False,\n",
       " 'always_save_checkpoint': False,\n",
       " 'init_from': 'pretrained:out/stories260k_default/ckpt.pt',\n",
       " 'wandb_log': True,\n",
       " 'wandb_project': 'llamac',\n",
       " 'wandb_run_name': 'tinyshakespeare_lora_stories260k_default_nodropout_0822_1536',\n",
       " 'batch_size': 128,\n",
       " 'max_seq_len': 512,\n",
       " 'vocab_source': 'custom',\n",
       " 'vocab_size': 512,\n",
       " 'dataset': 'tinyshakespeare',\n",
       " 'dim': 288,\n",
       " 'n_layers': 6,\n",
       " 'n_heads': 6,\n",
       " 'n_kv_heads': 6,\n",
       " 'multiple_of': 32,\n",
       " 'dropout': 0.0,\n",
       " 'use_lora': True,\n",
       " 'lora_rank': 16,\n",
       " 'lora_alpha': 1,\n",
       " 'lora_dropout_p': 0.0,\n",
       " 'gradient_accumulation_steps': 1,\n",
       " 'learning_rate': 0.001,\n",
       " 'max_iters': 20000,\n",
       " 'weight_decay': 0.01,\n",
       " 'beta1': 0.9,\n",
       " 'beta2': 0.99,\n",
       " 'grad_clip': 1.0,\n",
       " 'decay_lr': True,\n",
       " 'warmup_iters': 1000,\n",
       " 'device': 'cuda',\n",
       " 'dtype': 'bfloat16',\n",
       " 'compile': True}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt[\"config\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### inspect the tiny story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m = np.memmap(\"./data/tok512/data00.bin\", dtype=np.uint16, mode=\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "memmap([  1, 317, 269, ..., 287, 411, 426], dtype=uint16)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"./data/TinyStories_all_data/data00.json\", \"r\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Lily and Ben are friends. They like to play in the park. One day, they see a big tree with a swing. Lily wants to try the swing. She runs to the tree and climbs on the swing.\n",
      "\"Push me, Ben!\" she says. Ben pushes her gently. Lily feels happy. She swings higher and higher. She laughs and shouts.\n",
      "Ben watches Lily. He thinks she is cute. He wants to swing too. He waits for Lily to stop. But Lily does not stop. She swings faster and faster. She is having too much fun.\n",
      "\"Can I swing too, Lily?\" Ben asks. Lily does not hear him. She is too busy swinging. Ben feels sad. He walks away.\n",
      "Lily swings so high that she loses her grip. She falls off the swing. She lands on the ground. She hurts her foot. She cries.\n",
      "\"Ow, ow, ow!\" she says. She looks for Ben. She wants him to help her. But Ben is not there. He is gone.\n",
      "Lily feels sorry. She wishes she had shared the swing with Ben. She wishes he was there to hug her. She limps to the tree. She sees something hanging from a branch. It is Ben's hat. He left it for her.\n",
      "Lily smiles. She thinks Ben is nice. She puts on his hat. She hopes he will come back. She wants to say sorry. She wants to be friends again.\n"
     ]
    }
   ],
   "source": [
    "print(data[0]['story'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tok = Tokenizer(\"./data/tok512.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lily and Ben are friends. They like to play in the park. One day, they see a big tree with a swing. Lily wants to try the swing. She runs to the tree and climbs on the swing.\n",
      "\"Push me, Ben!\" she says. Ben pushes her gently. Lily feels happy. She swings higher and higher. She laughs and shouts.\n",
      "Ben watches Lily. He thinks she is cute. He wants to swing too. He waits for Lily to stop. But Lily does not stop. She swings faster and faster. She is having too much fun.\n",
      "\"Can I swing too, Lily?\" Ben asks. Lily does not hear him. She is too busy swinging. Ben feels sad. He walks away.\n",
      "Lily swings so high that she loses her grip. She falls off the swing. She lands on the ground. She hurts her foot. She cries.\n",
      "\"Ow, ow, ow!\" she says. She looks for Ben. She wants him to help her. But Ben is not there. He is gone.\n",
      "Lily feels sorry. She wishes she had shared the swing with Ben. She wishes he was there to hug her. She limps to the tree. She sees something hanging from a branch. It is Ben's hat. He left it for her.\n",
      "Lily smiles. She thinks Ben is nice. She puts on his hat. She hopes he will come back. She wants to say sorry. She wants to be friends again. Once upon a time, there was a little girl named Lily. She had a teddy bear that she loved so much. One day, she lost it while playing in the park. She looked everywhere, but she couldn't find it. She felt sad and scared without her teddy bear. \n",
      "Lily's mommy saw her crying and asked what was wrong. Lily told her that she lost her teddy bear. Mommy hugged her and said, \"Don't worry, we'll search for it together.\" They went back to the park and looked everywhere. After a while, they found the teddy bear under a tree. Lily was so happy! \n",
      "She hugged her teddy bear and felt comfortable again. She said, \"I hope I never lose you again, teddy bear.\" Mommy smiled and said, \"Me too, Lily. You and teddy bear are the best of friends.\" And they all went home, happy and content. The end. Once upon a time, there was a little girl named Lily. She had an idea to make a big tower of blocks. She pulled all the blocks together and started building. She was very happy as the tower\n"
     ]
    }
   ],
   "source": [
    "print(tok.decode(m.tolist()[:1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prepare tinyshakespeare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. encode the whole text\n",
    "2. save it into a .bin\n",
    "3. dataloader load it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
