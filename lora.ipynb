{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%autoreload` not found.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from functools import partial\n",
    "from model import Transformer, ModelArgs\n",
    "from lora import add_lora, remove_lora\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (tok_embeddings): Embedding(32000, 512)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (layers): ModuleList(\n",
      "    (0-7): 8 x TransformerBlock(\n",
      "      (attention): Attention(\n",
      "        (wq): Linear(in_features=512, out_features=512, bias=False)\n",
      "        (wk): Linear(in_features=512, out_features=512, bias=False)\n",
      "        (wv): Linear(in_features=512, out_features=512, bias=False)\n",
      "        (wo): Linear(in_features=512, out_features=512, bias=False)\n",
      "        (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (feed_forward): FeedForward(\n",
      "        (w1): Linear(in_features=512, out_features=1376, bias=False)\n",
      "        (w2): Linear(in_features=1376, out_features=512, bias=False)\n",
      "        (w3): Linear(in_features=512, out_features=1376, bias=False)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (attention_norm): RMSNorm()\n",
      "      (ffn_norm): RMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): RMSNorm()\n",
      "  (output): Linear(in_features=512, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# stories 42M\n",
    "dim = 512\n",
    "n_layers = 8\n",
    "n_heads = 8\n",
    "n_kv_heads = 8\n",
    "multiple_of = 32\n",
    "dropout = 0.0\n",
    "vocab_size = 32000\n",
    "max_seq_len = 1024\n",
    "\n",
    "model_args = dict(\n",
    "    dim=dim,\n",
    "    n_layers=n_layers,\n",
    "    n_heads=n_heads,\n",
    "    n_kv_heads=n_kv_heads,\n",
    "    vocab_size=vocab_size,\n",
    "    multiple_of=multiple_of,\n",
    "    max_seq_len=max_seq_len,\n",
    "    dropout=dropout,\n",
    ")\n",
    "\n",
    "gptconf = ModelArgs(**model_args)\n",
    "model = Transformer(gptconf)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add lora to layers.0.attention.wq\n",
      "add lora to layers.0.attention.wk\n",
      "add lora to layers.1.attention.wq\n",
      "add lora to layers.1.attention.wk\n",
      "add lora to layers.2.attention.wq\n",
      "add lora to layers.2.attention.wk\n",
      "add lora to layers.3.attention.wq\n",
      "add lora to layers.3.attention.wk\n",
      "add lora to layers.4.attention.wq\n",
      "add lora to layers.4.attention.wk\n",
      "add lora to layers.5.attention.wq\n",
      "add lora to layers.5.attention.wk\n",
      "add lora to layers.6.attention.wq\n",
      "add lora to layers.6.attention.wk\n",
      "add lora to layers.7.attention.wq\n",
      "add lora to layers.7.attention.wk\n",
      "Transformer(\n",
      "  (tok_embeddings): Embedding(32000, 512)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (layers): ModuleList(\n",
      "    (0-7): 8 x TransformerBlock(\n",
      "      (attention): Attention(\n",
      "        (wq): ParametrizedLinear(\n",
      "          in_features=512, out_features=512, bias=False\n",
      "          (parametrizations): ModuleDict(\n",
      "            (weight): ParametrizationList(\n",
      "              (0): LoRA(in_features=512, out_features=512, weight_type=linear, lora_rank=8, lora_alpha=16, lora_dropout_p=0.05\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (wk): ParametrizedLinear(\n",
      "          in_features=512, out_features=512, bias=False\n",
      "          (parametrizations): ModuleDict(\n",
      "            (weight): ParametrizationList(\n",
      "              (0): LoRA(in_features=512, out_features=512, weight_type=linear, lora_rank=8, lora_alpha=16, lora_dropout_p=0.05\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (wv): Linear(in_features=512, out_features=512, bias=False)\n",
      "        (wo): Linear(in_features=512, out_features=512, bias=False)\n",
      "        (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (feed_forward): FeedForward(\n",
      "        (w1): Linear(in_features=512, out_features=1376, bias=False)\n",
      "        (w2): Linear(in_features=1376, out_features=512, bias=False)\n",
      "        (w3): Linear(in_features=512, out_features=1376, bias=False)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (attention_norm): RMSNorm()\n",
      "      (ffn_norm): RMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): RMSNorm()\n",
      "  (output): Linear(in_features=512, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "lora_rank = 8\n",
    "lora_alpha = 16\n",
    "lora_dropout_p = 0.05\n",
    "target_modules = ['wq', 'wk']\n",
    "\n",
    "remove_lora(model)\n",
    "add_lora(model, rank=lora_rank, alpha=lora_alpha, dropout_p=lora_dropout_p, target_modules=target_modules)\n",
    "print(model)\n",
    "# register_lora_layer(model, lora_rank, lora_alpha, lora_dropout_p, target_modules)\n",
    "# model.apply(partial(register_lora_layer, lora_rank=lora_rank, lora_alpha=lora_alpha, lora_dropout_p=lora_dropout_p, target_modules=target_modules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lora_params(model, print_shapes=True):\n",
    "    def name_is_lora(name):\n",
    "        return (\n",
    "            len(name.split(\".\")) >= 4\n",
    "            and (name.split(\".\")[-4]) == \"parametrizations\"\n",
    "            and name.split(\".\")[-1] in [\"lora_A\", \"lora_B\"]\n",
    "        )\n",
    "    for n, p in model.named_parameters():\n",
    "        if name_is_lora(n):\n",
    "            if print_shapes:\n",
    "                print(n, p.shape)\n",
    "            yield p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.attention.wq.parametrizations.weight.0.lora_A torch.Size([512, 8])\n",
      "layers.0.attention.wq.parametrizations.weight.0.lora_B torch.Size([8, 512])\n",
      "layers.0.attention.wk.parametrizations.weight.0.lora_A torch.Size([512, 8])\n",
      "layers.0.attention.wk.parametrizations.weight.0.lora_B torch.Size([8, 512])\n",
      "layers.1.attention.wq.parametrizations.weight.0.lora_A torch.Size([512, 8])\n",
      "layers.1.attention.wq.parametrizations.weight.0.lora_B torch.Size([8, 512])\n",
      "layers.1.attention.wk.parametrizations.weight.0.lora_A torch.Size([512, 8])\n",
      "layers.1.attention.wk.parametrizations.weight.0.lora_B torch.Size([8, 512])\n",
      "layers.2.attention.wq.parametrizations.weight.0.lora_A torch.Size([512, 8])\n",
      "layers.2.attention.wq.parametrizations.weight.0.lora_B torch.Size([8, 512])\n",
      "layers.2.attention.wk.parametrizations.weight.0.lora_A torch.Size([512, 8])\n",
      "layers.2.attention.wk.parametrizations.weight.0.lora_B torch.Size([8, 512])\n",
      "layers.3.attention.wq.parametrizations.weight.0.lora_A torch.Size([512, 8])\n",
      "layers.3.attention.wq.parametrizations.weight.0.lora_B torch.Size([8, 512])\n",
      "layers.3.attention.wk.parametrizations.weight.0.lora_A torch.Size([512, 8])\n",
      "layers.3.attention.wk.parametrizations.weight.0.lora_B torch.Size([8, 512])\n",
      "layers.4.attention.wq.parametrizations.weight.0.lora_A torch.Size([512, 8])\n",
      "layers.4.attention.wq.parametrizations.weight.0.lora_B torch.Size([8, 512])\n",
      "layers.4.attention.wk.parametrizations.weight.0.lora_A torch.Size([512, 8])\n",
      "layers.4.attention.wk.parametrizations.weight.0.lora_B torch.Size([8, 512])\n",
      "layers.5.attention.wq.parametrizations.weight.0.lora_A torch.Size([512, 8])\n",
      "layers.5.attention.wq.parametrizations.weight.0.lora_B torch.Size([8, 512])\n",
      "layers.5.attention.wk.parametrizations.weight.0.lora_A torch.Size([512, 8])\n",
      "layers.5.attention.wk.parametrizations.weight.0.lora_B torch.Size([8, 512])\n",
      "layers.6.attention.wq.parametrizations.weight.0.lora_A torch.Size([512, 8])\n",
      "layers.6.attention.wq.parametrizations.weight.0.lora_B torch.Size([8, 512])\n",
      "layers.6.attention.wk.parametrizations.weight.0.lora_A torch.Size([512, 8])\n",
      "layers.6.attention.wk.parametrizations.weight.0.lora_B torch.Size([8, 512])\n",
      "layers.7.attention.wq.parametrizations.weight.0.lora_A torch.Size([512, 8])\n",
      "layers.7.attention.wq.parametrizations.weight.0.lora_B torch.Size([8, 512])\n",
      "layers.7.attention.wk.parametrizations.weight.0.lora_A torch.Size([512, 8])\n",
      "layers.7.attention.wk.parametrizations.weight.0.lora_B torch.Size([8, 512])\n"
     ]
    }
   ],
   "source": [
    "for n in get_lora_params(model):\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ckpt = torch.load(\"./out/tinyshakespeare_lora_stories260k_default_nodropout_0822_1536/ckpt.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'out_dir': 'out/tinyshakespeare_lora_stories260k_default_nodropout_0822_1536',\n",
       " 'eval_interval': 2000,\n",
       " 'log_interval': 1,\n",
       " 'eval_iters': 100,\n",
       " 'eval_only': False,\n",
       " 'always_save_checkpoint': False,\n",
       " 'init_from': 'pretrained:out/stories260k_default/ckpt.pt',\n",
       " 'wandb_log': True,\n",
       " 'wandb_project': 'llamac',\n",
       " 'wandb_run_name': 'tinyshakespeare_lora_stories260k_default_nodropout_0822_1536',\n",
       " 'batch_size': 128,\n",
       " 'max_seq_len': 512,\n",
       " 'vocab_source': 'custom',\n",
       " 'vocab_size': 512,\n",
       " 'dataset': 'tinyshakespeare',\n",
       " 'dim': 288,\n",
       " 'n_layers': 6,\n",
       " 'n_heads': 6,\n",
       " 'n_kv_heads': 6,\n",
       " 'multiple_of': 32,\n",
       " 'dropout': 0.0,\n",
       " 'use_lora': True,\n",
       " 'lora_rank': 16,\n",
       " 'lora_alpha': 1,\n",
       " 'lora_dropout_p': 0.0,\n",
       " 'gradient_accumulation_steps': 1,\n",
       " 'learning_rate': 0.001,\n",
       " 'max_iters': 20000,\n",
       " 'weight_decay': 0.01,\n",
       " 'beta1': 0.9,\n",
       " 'beta2': 0.99,\n",
       " 'grad_clip': 1.0,\n",
       " 'decay_lr': True,\n",
       " 'warmup_iters': 1000,\n",
       " 'device': 'cuda',\n",
       " 'dtype': 'bfloat16',\n",
       " 'compile': True}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt[\"config\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### inspect the tiny story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m = np.memmap(\"./data/tok512/data00.bin\", dtype=np.uint16, mode=\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "memmap([  1, 317, 269, ..., 287, 411, 426], dtype=uint16)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"./data/TinyStories_all_data/data00.json\", \"r\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Lily and Ben are friends. They like to play in the park. One day, they see a big tree with a swing. Lily wants to try the swing. She runs to the tree and climbs on the swing.\n",
      "\"Push me, Ben!\" she says. Ben pushes her gently. Lily feels happy. She swings higher and higher. She laughs and shouts.\n",
      "Ben watches Lily. He thinks she is cute. He wants to swing too. He waits for Lily to stop. But Lily does not stop. She swings faster and faster. She is having too much fun.\n",
      "\"Can I swing too, Lily?\" Ben asks. Lily does not hear him. She is too busy swinging. Ben feels sad. He walks away.\n",
      "Lily swings so high that she loses her grip. She falls off the swing. She lands on the ground. She hurts her foot. She cries.\n",
      "\"Ow, ow, ow!\" she says. She looks for Ben. She wants him to help her. But Ben is not there. He is gone.\n",
      "Lily feels sorry. She wishes she had shared the swing with Ben. She wishes he was there to hug her. She limps to the tree. She sees something hanging from a branch. It is Ben's hat. He left it for her.\n",
      "Lily smiles. She thinks Ben is nice. She puts on his hat. She hopes he will come back. She wants to say sorry. She wants to be friends again.\n"
     ]
    }
   ],
   "source": [
    "print(data[0]['story'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tok = Tokenizer(\"./data/tok512.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lily and Ben are friends. They like to play in the park. One day, they see a big tree with a swing. Lily wants to try the swing. She runs to the tree and climbs on the swing.\n",
      "\"Push me, Ben!\" she says. Ben pushes her gently. Lily feels happy. She swings higher and higher. She laughs and shouts.\n",
      "Ben watches Lily. He thinks she is cute. He wants to swing too. He waits for Lily to stop. But Lily does not stop. She swings faster and faster. She is having too much fun.\n",
      "\"Can I swing too, Lily?\" Ben asks. Lily does not hear him. She is too busy swinging. Ben feels sad. He walks away.\n",
      "Lily swings so high that she loses her grip. She falls off the swing. She lands on the ground. She hurts her foot. She cries.\n",
      "\"Ow, ow, ow!\" she says. She looks for Ben. She wants him to help her. But Ben is not there. He is gone.\n",
      "Lily feels sorry. She wishes she had shared the swing with Ben. She wishes he was there to hug her. She limps to the tree. She sees something hanging from a branch. It is Ben's hat. He left it for her.\n",
      "Lily smiles. She thinks Ben is nice. She puts on his hat. She hopes he will come back. She wants to say sorry. She wants to be friends again. Once upon a time, there was a little girl named Lily. She had a teddy bear that she loved so much. One day, she lost it while playing in the park. She looked everywhere, but she couldn't find it. She felt sad and scared without her teddy bear. \n",
      "Lily's mommy saw her crying and asked what was wrong. Lily told her that she lost her teddy bear. Mommy hugged her and said, \"Don't worry, we'll search for it together.\" They went back to the park and looked everywhere. After a while, they found the teddy bear under a tree. Lily was so happy! \n",
      "She hugged her teddy bear and felt comfortable again. She said, \"I hope I never lose you again, teddy bear.\" Mommy smiled and said, \"Me too, Lily. You and teddy bear are the best of friends.\" And they all went home, happy and content. The end. Once upon a time, there was a little girl named Lily. She had an idea to make a big tower of blocks. She pulled all the blocks together and started building. She was very happy as the tower\n"
     ]
    }
   ],
   "source": [
    "print(tok.decode(m.tolist()[:1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prepare tinyshakespeare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. encode the whole text\n",
    "2. save it into a .bin\n",
    "3. dataloader load it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## register_parametrization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.parametrize as parametrize\n",
    "import torch.autograd.profiler as profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(300, 300)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "\n",
    "class LoRA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lora_A = nn.Parameter(torch.zeros(300, 300))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(300, 300))\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B)\n",
    "        self.dropout = nn.Dropout(0.01)\n",
    "        self.register_buffer(\n",
    "            \"lora_dropout_mask\", torch.ones(300, 1, dtype=self.lora_A.dtype)\n",
    "        )\n",
    "        \n",
    "    def forward(self, weight):\n",
    "        print(\"foward!\")\n",
    "        dropout_lora_A = self.dropout(self.lora_dropout_mask) * self.lora_A\n",
    "        delta = dropout_lora_A @ self.lora_B\n",
    "        return weight + delta.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu used 9933312 memory\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.reset_peak_memory_stats(device=None)\n",
    "model = Model()\n",
    "model.cuda()\n",
    "x = torch.randn(1, 300).cuda()\n",
    "out = model(x)\n",
    "print(f\"gpu used {torch.cuda.max_memory_allocated(device=None)} memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foward!\n",
      "foward!\n",
      "gpu used 11378176 memory\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.reset_peak_memory_stats(device=None)\n",
    "model = Model()\n",
    "parametrize.register_parametrization(model.linear, \"weight\", LoRA())\n",
    "model.cuda()\n",
    "x = torch.randn(1, 300).cuda()\n",
    "out = model(x)\n",
    "print(f\"gpu used {torch.cuda.max_memory_allocated(device=None)} memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foward!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ParametrizedLinear(\n",
       "  in_features=3, out_features=3, bias=True\n",
       "  (parametrizations): ModuleDict(\n",
       "    (weight): ParametrizationList(\n",
       "      (0): Symmetric()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model()\n",
    "model.cuda()\n",
    "parametrize.register_parametrization(model.linear2, \"weight\", Symmetric())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear1.weight',\n",
       "              tensor([[ 0.0129, -0.2169,  0.3881],\n",
       "                      [-0.5198,  0.2688, -0.4700],\n",
       "                      [ 0.0152,  0.1100,  0.1116]], device='cuda:0')),\n",
       "             ('linear1.bias',\n",
       "              tensor([-0.0981,  0.1367,  0.0430], device='cuda:0')),\n",
       "             ('linear2.bias',\n",
       "              tensor([0.3150, 0.5501, 0.5367], device='cuda:0')),\n",
       "             ('linear2.parametrizations.weight.original',\n",
       "              tensor([[ 0.1634,  0.5439,  0.1278],\n",
       "                      [ 0.3339,  0.2044, -0.2386],\n",
       "                      [-0.0448, -0.5470, -0.0414]], device='cuda:0'))])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = torch.randn(1, 3).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foward!\n",
      "foward!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2023-08-26 15:07:46 9482:9482 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-08-26 15:07:46 9482:9482 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-08-26 15:07:46 9482:9482 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "# warmup\n",
    "model(x)\n",
    "\n",
    "with profiler.profile(profile_memory=True, use_cuda=True) as prof:\n",
    "    out = model(x)\n",
    "    # out = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                           LINEAR PASS1        23.75%     351.000us        44.25%     654.000us     654.000us     355.000us        23.91%     668.000us     668.000us           0 b           0 b         512 b           0 b             1  \n",
      "                                           aten::linear         7.04%     104.000us        36.33%     537.000us     268.500us      82.000us         5.52%     557.000us     278.500us           0 b           0 b       1.00 Kb           0 b             2  \n",
      "                                                aten::t         5.14%      76.000us         9.27%     137.000us      68.500us      78.000us         5.25%     158.000us      79.000us           0 b           0 b           0 b           0 b             2  \n",
      "                                        aten::transpose         5.82%      86.000us         6.63%      98.000us      32.667us      81.000us         5.45%     126.000us      42.000us           0 b           0 b           0 b           0 b             3  \n",
      "                                       aten::as_strided         0.81%      12.000us         0.81%      12.000us       4.000us      45.000us         3.03%      45.000us      15.000us           0 b           0 b           0 b           0 b             3  \n",
      "                                            aten::addmm        15.49%     229.000us        20.03%     296.000us     148.000us     273.000us        18.38%     317.000us     158.500us           0 b           0 b       1.00 Kb      -2.00 Mb             2  \n",
      "                                            aten::empty         1.56%      23.000us         1.56%      23.000us      11.500us      44.000us         2.96%      44.000us      22.000us           0 b           0 b       2.00 Mb       2.00 Mb             2  \n",
      "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         0.47%       7.000us         0.47%       7.000us       3.500us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             2  \n",
      "                                       cudaLaunchKernel         4.94%      73.000us         4.94%      73.000us      14.600us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             5  \n",
      "                                           LINEAR PASS2        15.49%     229.000us        54.60%     807.000us     807.000us     217.000us        14.61%     817.000us     817.000us           0 b           0 b       1.00 Kb           0 b             1  \n",
      "                                       PARAMETERIZATION        11.37%     168.000us        23.27%     344.000us     344.000us     138.000us         9.29%     356.000us     356.000us           0 b           0 b         512 b      -1.00 Kb             1  \n",
      "                                             aten::triu         3.99%      59.000us         5.75%      85.000us      42.500us     107.000us         7.21%     107.000us      53.500us           0 b           0 b       1.00 Kb       1.00 Kb             2  \n",
      "                                              aten::add         2.98%      44.000us         3.65%      54.000us      54.000us      65.000us         4.38%      65.000us      65.000us           0 b           0 b         512 b         512 b             1  \n",
      "                                               [memory]         0.00%       0.000us         0.00%       0.000us       0.000us       0.000us         0.00%       0.000us       0.000us           0 b           0 b      -1.00 Kb      -1.00 Kb             2  \n",
      "                                  cudaDeviceSynchronize         1.15%      17.000us         1.15%      17.000us      17.000us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.478ms\n",
      "Self CUDA time total: 1.485ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages(group_by_stack_n=5).table())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prof.export_chrome_trace(\"trace.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu used 2560 memory\n"
     ]
    }
   ],
   "source": [
    "print(f\"gpu used {torch.cuda.max_memory_allocated(device=None)} memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
